{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in train and test as Pandas DataFrames\n",
    "\"\"\"\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "# df_train.head()\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store gap values\n",
    "Y_train = df_train.gap.values\n",
    "#row where testing examples start\n",
    "test_idx = df_train.shape[0]\n",
    "#delete 'Id' column\n",
    "df_test = df_test.drop(['Id'], axis=1)\n",
    "#delete 'gap' column\n",
    "df_train = df_train.drop(['gap'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove useless features from the set\n",
    "UselessNames=['feat_002', 'feat_003', 'feat_004', 'feat_008', 'feat_009', 'feat_010', 'feat_011', 'feat_012', 'feat_013', 'feat_014', 'feat_015', 'feat_016', 'feat_017', 'feat_018', 'feat_019', 'feat_020', 'feat_021', 'feat_022', 'feat_023', 'feat_024', 'feat_026', 'feat_027', 'feat_028', 'feat_029', 'feat_030', 'feat_031', 'feat_032', 'feat_033', 'feat_034', 'feat_035', 'feat_036', 'feat_038', 'feat_039', 'feat_040', 'feat_041', 'feat_042', 'feat_043', 'feat_045', 'feat_046', 'feat_047', 'feat_048', 'feat_049', 'feat_050', 'feat_051', 'feat_052', 'feat_053', 'feat_054', 'feat_055', 'feat_056', 'feat_057', 'feat_058', 'feat_059', 'feat_060', 'feat_061', 'feat_062', 'feat_063', 'feat_064', 'feat_065', 'feat_066', 'feat_067', 'feat_070', 'feat_071', 'feat_073', 'feat_074', 'feat_075', 'feat_076', 'feat_077', 'feat_078', 'feat_079', 'feat_080', 'feat_081', 'feat_082', 'feat_083', 'feat_084', 'feat_085', 'feat_086', 'feat_088', 'feat_089', 'feat_091', 'feat_092', 'feat_093', 'feat_094', 'feat_095', 'feat_096', 'feat_097', 'feat_098', 'feat_099', 'feat_100', 'feat_101', 'feat_103', 'feat_104', 'feat_105', 'feat_106', 'feat_107', 'feat_108', 'feat_109', 'feat_110', 'feat_111', 'feat_112', 'feat_113', 'feat_114', 'feat_115', 'feat_116', 'feat_117', 'feat_118', 'feat_120', 'feat_121', 'feat_122', 'feat_124', 'feat_125', 'feat_127', 'feat_128', 'feat_129', 'feat_130', 'feat_131', 'feat_133', 'feat_134', 'feat_135', 'feat_136', 'feat_137', 'feat_138', 'feat_139', 'feat_140', 'feat_141', 'feat_142', 'feat_143', 'feat_144', 'feat_145', 'feat_146', 'feat_147', 'feat_148', 'feat_149', 'feat_150', 'feat_151', 'feat_152', 'feat_153', 'feat_154', 'feat_155', 'feat_156', 'feat_157', 'feat_158', 'feat_159', 'feat_160', 'feat_161', 'feat_162', 'feat_163', 'feat_164', 'feat_165', 'feat_166', 'feat_167', 'feat_168', 'feat_169', 'feat_170', 'feat_171', 'feat_172', 'feat_174', 'feat_175', 'feat_177', 'feat_178', 'feat_179', 'feat_180', 'feat_181', 'feat_182', 'feat_183', 'feat_184', 'feat_185', 'feat_186', 'feat_188', 'feat_189', 'feat_190', 'feat_191', 'feat_192', 'feat_193', 'feat_194', 'feat_195', 'feat_197', 'feat_198', 'feat_201', 'feat_202', 'feat_203', 'feat_204', 'feat_205', 'feat_206', 'feat_207', 'feat_209', 'feat_210', 'feat_211', 'feat_212', 'feat_213', 'feat_214', 'feat_215', 'feat_216', 'feat_217', 'feat_219', 'feat_220', 'feat_221', 'feat_222', 'feat_223', 'feat_224', 'feat_227', 'feat_228', 'feat_229', 'feat_230', 'feat_231', 'feat_232', 'feat_233', 'feat_234', 'feat_235', 'feat_236', 'feat_237', 'feat_238', 'feat_239', 'feat_240', 'feat_241', 'feat_242', 'feat_244', 'feat_245', 'feat_246', 'feat_247', 'feat_249', 'feat_250', 'feat_253', 'feat_254', 'feat_255', 'feat_256']\n",
    "\n",
    "df_train_clean = df_train.drop(UselessNames, axis=1)\n",
    "df_test_clean = df_test.drop(UselessNames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert all 0 values to 2's\n",
    "df_train_clean=df_train_clean.replace(0,2)\n",
    "df_test_clean=df_test_clean.replace(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scaller: Trying to make all values centered\n",
    "#disabled for now, as we are working on other tests\n",
    "\n",
    "# df_train2=df_train.drop(['smiles'],axis=1).head()\n",
    "\n",
    "# smiles=df_train['smiles']\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(df_train2)\n",
    "# df_train2=scaler.transform(df_train2) \n",
    "# df_train2=pd.DataFrame(data=df_train2[0:,0:])\n",
    "# #               index=df_train2[1:,0],\n",
    "# #               columns=df_train2[0,1:])\n",
    "# df_train2.insert(0, 'smiles', smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X Train Data Shape:', (800000, 31))\n",
      "('X Test Data Shape:', (200000, 31))\n",
      "('Y Train Data Shape:', (800000,))\n",
      "('Y Test Data Shape:', (200000,))\n"
     ]
    }
   ],
   "source": [
    "#Partitioning the data into training and test sets\n",
    "df_test_clean = df_test_clean.drop(['smiles'], axis=1)\n",
    "df_train_clean = df_train_clean.drop(['smiles'], axis=1)\n",
    "vals_test = df_test_clean.values\n",
    "vals_train = df_train_clean.values\n",
    "\n",
    "Test_num = 5;\n",
    "piecesize = int(len(df_train)/5);\n",
    "\n",
    "X_train_train = vals_train[:(Test_num-1)*piecesize]\n",
    "X_train_test = vals_train[(Test_num-1)*piecesize:]\n",
    "Y_train_train =  Y_train[:(Test_num-1)*piecesize]\n",
    "Y_train_test = Y_train[(Test_num-1)*piecesize:]\n",
    "\n",
    "X_train_train = preprocessing.scale(X_train_train.T)\n",
    "X_train_train = X_train_train.T\n",
    "X_train_test = preprocessing.scale(X_train_test.T)\n",
    "X_train_test = X_train_test.T\n",
    "\n",
    "print (\"X Train Data Shape:\", X_train_train.shape)\n",
    "print (\"X Test Data Shape:\", X_train_test.shape)\n",
    "print (\"Y Train Data Shape:\", Y_train_train.shape)\n",
    "print (\"Y Test Data Shape:\", Y_train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample Feature Engineering\\n\\nthis calculates the length of each smile string and adds a feature column with those lengths\\nNote: this is NOT a good feature and will result in a lower score!\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example Feature Engineering\n",
    "\n",
    "this calculates the length of each smile string and adds a feature column with those lengths\n",
    "Note: this is NOT a good feature and will result in a lower score!\n",
    "\"\"\"\n",
    "#smiles_len = np.vstack(df_all.smiles.astype(str).apply(lambda x: len(x)))\n",
    "#df_all['smiles_len'] = pd.DataFrame(smiles_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0204179287\n",
      "0.299252152197\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression \n",
    "LR = LinearRegression(n_jobs=-1)\n",
    "BT=time.time()\n",
    "LR.fit(X_train_train, Y_train_train)\n",
    "ET=time.time()\n",
    "LR_pred = LR.predict(X_train_test)\n",
    "\n",
    "LR_Err=RMSE(Y_train_test,LR_pred)\n",
    "\n",
    "print ET-BT\n",
    "print LR_Err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.5637931824\n",
      "0.273390285215\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regression\n",
    "RF = RandomForestRegressor(n_jobs=-1)\n",
    "BT=time.time()\n",
    "RF.fit(X_train_train, Y_train_train)\n",
    "ET=time.time()\n",
    "RF_pred = RF.predict(X_train_test)\n",
    "\n",
    "RF_Err=RMSE(Y_train_test,RF_pred)\n",
    "print ET-BT\n",
    "\n",
    "print RF_Err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.575790882\n",
      "0.299267569507\n"
     ]
    }
   ],
   "source": [
    "# Elastic Net Regression\n",
    "EN=ElasticNetCV(n_jobs=-1)\n",
    "BT=time.time()\n",
    "EN.fit(X_train_train, Y_train_train)\n",
    "ET=time.time()\n",
    "\n",
    "EN_pred = EN.predict(X_train_test)\n",
    "\n",
    "EN_Err=RMSE(Y_train_test,EN_pred)\n",
    "       \n",
    "print ET-BT\n",
    "print EN_Err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.592020988464\n",
      "0.407498540731\n"
     ]
    }
   ],
   "source": [
    "# Elastic Net Regression\n",
    "MN=ElasticNet(alpha=0.8)\n",
    "BT=time.time()\n",
    "MN.fit(X_train_train, Y_train_train)\n",
    "ET=time.time()\n",
    "\n",
    "MN_pred = MN.predict(X_train_test)\n",
    "\n",
    "MN_Err=RMSE(Y_train_test,MN_pred)\n",
    "print ET-BT\n",
    "print MN_Err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__getstate__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_param_names', '_transform', 'accept_sparse', 'fit', 'fit_transform', 'func', 'get_params', 'inv_kw_args', 'inverse_func', 'inverse_transform', 'kw_args', 'pass_y', 'set_params', 'transform', 'validate']\n",
      "19315.6042521\n"
     ]
    }
   ],
   "source": [
    "def basis_func_extension(X,d=10):\n",
    "    curr_train = np.ones(X.shape)\n",
    "    for i in np.arange(0, d):\n",
    "        curr_train = np.vstack([curr_train.T,np.sin(X*1./(i+1)).T]).T\n",
    "    return curr_train\n",
    "\n",
    "FT=preprocessing.FunctionTransformer(basis_func_extension(X_train))\n",
    "print dir(FT)\n",
    "BR=time.time()\n",
    "FT.fit(X_train,Y_train)\n",
    "ET=time.time()\n",
    "\n",
    "# FT_pred = FT.predict(X_test)\n",
    "\n",
    "\n",
    "print ET-BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN Score = 0.463186214763\n",
      "RF Score = 0.641257258918\n",
      "LR Score = 0.461816178672\n",
      "MN Score = 0.00749168031858\n"
     ]
    }
   ],
   "source": [
    "print 'EN Score =',EN.score(X_train,Y_train)\n",
    "print 'RF Score =',RF.score(X_train,Y_train)\n",
    "print 'LR Score =',LR.score(X_train,Y_train)\n",
    "print 'MN Score =',MN.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
       "       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,\n",
       "       normalize=False, positive=False, precompute='auto',\n",
       "       random_state=None, selection='cyclic', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, predictions):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i,p in enumerate(predictions):\n",
    "            f.write(str(i+1) + \",\" + str(p) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file(\"sample1.csv\", LR_pred)\n",
    "write_to_file(\"sample2.csv\", RF_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RMSE(Y,Y_Pred):\n",
    "    N=len(Y)\n",
    "    Err=0\n",
    "    for i in xrange(N):\n",
    "        Err+=(Y[i]-Y_Pred[i])**2\n",
    "    Err=(1./N*Err)**(1./2.)\n",
    "    return Err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names2=['feat_002', 'feat_003', 'feat_004', 'feat_008', 'feat_009', 'feat_010', 'feat_011', 'feat_012', 'feat_013', 'feat_014', 'feat_015', 'feat_016', 'feat_017', 'feat_018', 'feat_019', 'feat_020', 'feat_021', 'feat_022', 'feat_023', 'feat_024', 'feat_026', 'feat_027', 'feat_028', 'feat_029', 'feat_030', 'feat_031', 'feat_032', 'feat_033', 'feat_034', 'feat_035', 'feat_036', 'feat_038', 'feat_039', 'feat_040', 'feat_041', 'feat_042', 'feat_043', 'feat_045', 'feat_046', 'feat_047', 'feat_048', 'feat_049', 'feat_050', 'feat_051', 'feat_052', 'feat_053', 'feat_054', 'feat_055', 'feat_056', 'feat_057', 'feat_058', 'feat_059', 'feat_060', 'feat_061', 'feat_062', 'feat_063', 'feat_064', 'feat_065', 'feat_066', 'feat_067', 'feat_070', 'feat_071', 'feat_073', 'feat_074', 'feat_075', 'feat_076', 'feat_077', 'feat_078', 'feat_079', 'feat_080', 'feat_081', 'feat_082', 'feat_083', 'feat_084', 'feat_085', 'feat_086', 'feat_088', 'feat_089', 'feat_091', 'feat_092', 'feat_093', 'feat_094', 'feat_095', 'feat_096', 'feat_097', 'feat_098', 'feat_099', 'feat_100', 'feat_101', 'feat_103', 'feat_104', 'feat_105', 'feat_106', 'feat_107', 'feat_108', 'feat_109', 'feat_110', 'feat_111', 'feat_112', 'feat_113', 'feat_114', 'feat_115', 'feat_116', 'feat_117', 'feat_118', 'feat_120', 'feat_121', 'feat_122', 'feat_124', 'feat_125', 'feat_127', 'feat_128', 'feat_129', 'feat_130', 'feat_131', 'feat_133', 'feat_134', 'feat_135', 'feat_136', 'feat_137', 'feat_138', 'feat_139', 'feat_140', 'feat_141', 'feat_142', 'feat_143', 'feat_144', 'feat_145', 'feat_146', 'feat_147', 'feat_148', 'feat_149', 'feat_150', 'feat_151', 'feat_152', 'feat_153', 'feat_154', 'feat_155', 'feat_156', 'feat_157', 'feat_158', 'feat_159', 'feat_160', 'feat_161', 'feat_162', 'feat_163', 'feat_164', 'feat_165', 'feat_166', 'feat_167', 'feat_168', 'feat_169', 'feat_170', 'feat_171', 'feat_172', 'feat_174', 'feat_175', 'feat_177', 'feat_178', 'feat_179', 'feat_180', 'feat_181', 'feat_182', 'feat_183', 'feat_184', 'feat_185', 'feat_186', 'feat_188', 'feat_189', 'feat_190', 'feat_191', 'feat_192', 'feat_193', 'feat_194', 'feat_195', 'feat_197', 'feat_198', 'feat_201', 'feat_202', 'feat_203', 'feat_204', 'feat_205', 'feat_206', 'feat_207', 'feat_209', 'feat_210', 'feat_211', 'feat_212', 'feat_213', 'feat_214', 'feat_215', 'feat_216', 'feat_217', 'feat_219', 'feat_220', 'feat_221', 'feat_222', 'feat_223', 'feat_224', 'feat_227', 'feat_228', 'feat_229', 'feat_230', 'feat_231', 'feat_232', 'feat_233', 'feat_234', 'feat_235', 'feat_236', 'feat_237', 'feat_238', 'feat_239', 'feat_240', 'feat_241', 'feat_242', 'feat_244', 'feat_245', 'feat_246', 'feat_247', 'feat_249', 'feat_250', 'feat_253', 'feat_254', 'feat_255', 'feat_256']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_curr = df_all.drop(names2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>feat_010</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_247</th>\n",
       "      <th>feat_248</th>\n",
       "      <th>feat_249</th>\n",
       "      <th>feat_250</th>\n",
       "      <th>feat_251</th>\n",
       "      <th>feat_252</th>\n",
       "      <th>feat_253</th>\n",
       "      <th>feat_254</th>\n",
       "      <th>feat_255</th>\n",
       "      <th>feat_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_001  feat_002  feat_003  feat_004  feat_005  feat_006  feat_007  \\\n",
       "0       0.0       0.0       0.0       0.0       1.0       0.0       1.0   \n",
       "1       1.0       0.0       0.0       0.0       1.0       0.0       1.0   \n",
       "2       1.0       0.0       0.0       0.0       1.0       1.0       1.0   \n",
       "3       1.0       0.0       0.0       0.0       1.0       1.0       1.0   \n",
       "4       0.0       0.0       0.0       0.0       1.0       0.0       1.0   \n",
       "\n",
       "   feat_008  feat_009  feat_010    ...     feat_247  feat_248  feat_249  \\\n",
       "0       0.0       0.0       0.0    ...          0.0       1.0       0.0   \n",
       "1       0.0       0.0       0.0    ...          0.0       1.0       0.0   \n",
       "2       0.0       0.0       0.0    ...          0.0       1.0       0.0   \n",
       "3       0.0       0.0       0.0    ...          0.0       1.0       0.0   \n",
       "4       0.0       0.0       0.0    ...          0.0       1.0       0.0   \n",
       "\n",
       "   feat_250  feat_251  feat_252  feat_253  feat_254  feat_255  feat_256  \n",
       "0       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "1       0.0       1.0       0.0       0.0       0.0       0.0       0.0  \n",
       "2       0.0       0.0       1.0       0.0       0.0       0.0       0.0  \n",
       "3       0.0       0.0       1.0       0.0       0.0       0.0       0.0  \n",
       "4       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 256 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def basis_func_extension(X,d=10):\n",
    "    curr_train = np.ones(X.shape)\n",
    "    for i in np.arange(0, d):\n",
    "        curr_train = np.vstack([curr_train.T,np.sin(X*1./(i+1)).T]).T\n",
    "    return curr_train\n",
    "\n",
    "FT=preprocessing.FunctionTransformer(basis_func_extension(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
